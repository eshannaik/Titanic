# -*- coding: utf-8 -*-
"""Copy of Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fOlFYfmJ2lYfTmsaNyUDXQBxf2xZ5ulM

#Titanic Survival Prediction

### Importing and understanding the data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import keras
from keras.models import Model,Sequential
from keras.layers import Dense,Dropout

# Reading the two Datasets
train = pd.read_csv("train.csv") 
test = pd.read_csv("test.csv")

train.head()

test.head()

train.info()

train.isnull().sum()

test.isnull().sum()

"""### Cleaning the Datasets"""

# Fix the missing data

a = train["Embarked"].mode()[0] # we fill the 2 null values from the Embarked column in the train dataset using the mode of the column
f = test["Fare"].mean() # we fill the 1 null value from the Fare column in the test dataset using the mean of the column

m = train['Age'].mean() # we fill the 86 null values from the Age column in the test dataset using the mode of the column
train['Age'].fillna(m,inplace=True)
# drop cabin as the missing rate for this column nearly 40% and the other 2 columns as they will not be needed
train.drop(["Cabin","Name","Ticket"],inplace=True,axis=1) 
train["Embarked"].fillna(a,inplace=True)


m1 = test['Age'].mean()
test['Age'].fillna(m1,inplace=True)
test.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)
test["Embarked"].fillna(a,inplace=True)
test["Fare"].fillna(f,inplace=True)

train.isna().sum()

test.isnull().sum()

train.describe()

train.shape

test.shape

train.nunique()

"""### Visualization

The below graph tells us that during the Titanic incident only 38.5 % percent of people survived, that is, little over every 1 in 3 persons surviving the incident.
"""

fig = plt.figure(figsize=(6,4))
ax = sns.countplot(data = train,x="Survived", palette="Set2")

total = 889

for p,label in zip(ax.patches,l1):
  percentage = '{:.1f}%'.format(100 * p.get_height()/total)
  x = p.get_x() + p.get_width() - 0.48
  y = p.get_y() + p.get_height()
  ax.annotate(percentage,(x,y))

"""Here we get to know that most of the passengers were male on the titanic."""

fig = plt.figure(figsize=(6,4))
ax = sns.countplot(data = train,y="Sex", palette="Set3")

total = 889

for p,label in zip(ax.patches,l1):
  percentage = '{:.1f}%'.format(100 * p.get_width()/total)
  x = p.get_x() + p.get_width() + 0.5
  y = p.get_y() + p.get_height()/2 
  ax.annotate(percentage,(x,y))

"""From the below histograms we can analyse that-

*   Most of the people who did not survive were from the 3rd ticket class and were between the age of 24-30.
*   Most of the people who survived were from the 1st ticket class and were between the age of 25-31.

"""

train.groupby('Survived').hist(figsize=(18, 18))

"""Here we see the mean survive rate depending on fare,class,age and embarkment."""

train['FareGrouping'] = pd.qcut(train['Fare'], 5)
train[["FareGrouping","Survived"]].groupby(['FareGrouping'],as_index=False).mean().sort_values(by="FareGrouping")

train[["Pclass","Survived"]].groupby(['Pclass'],as_index=False).mean().sort_values(by="Pclass")

train['AgeGrouping'] = pd.qcut(train['Age'], 5)
train[["AgeGrouping","Survived"]].groupby(['AgeGrouping'],as_index=False).mean().sort_values(by="AgeGrouping")

train[["Embarked","Survived"]].groupby(['Embarked'],as_index=False).mean().sort_values(by="Embarked")

train.drop(["FareGrouping","AgeGrouping"],inplace=True,axis=1)

train.shape

"""### Preprocessing

Here we label encode both the sex column and the Embark column to convert these categorical columns into numerical columns
"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
train["Sex"]=le.fit_transform(train["Sex"])
test["Sex"] = le.fit_transform(test["Sex"])
train["Embarked"]=le.fit_transform(train["Embarked"])
test["Embarked"] = le.fit_transform(test["Embarked"])
train.head()

train.head()

test.head()

"""## Models

### Percentage
"""

def percentage(y_pred):
  survived = 0
  not_survived = 0
  for i in range (0,418):
    if (y_pred[i] == 1) :
      survived = survived + 1
    else :
      not_survived = not_survived +1

  total=418
  p_survived = survived/total * 100
  p_not_survived = not_survived/total * 100

  print("Percentage of people predicted to survive",p_survived)
  print("Percentage of people predicted to not survive",p_not_survived)

"""### Dummy Classifier

The dummy classifier is used as a baseline model
"""

from sklearn.dummy import DummyClassifier
dummy=DummyClassifier(random_state=0)
dummy.fit(X,y)

y_pred_dummy = dummy.predict(test)

percentage(y_pred_dummy)

"""### Artificial Neural Network"""

X = train.drop(["Survived"],axis=1)
y = train["Survived"]

"""We split the training data in a 70/30 percent manner"""

X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.3)

model = Sequential()

model.add(Dense(units = 32, activation='relu', input_dim = 8))
model.add(Dense(units = 16, activation='relu'))
model.add(Dense(units = 1, activation = 'sigmoid'))

model.compile(optimizer="adam",loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, batch_size = 64, epochs = 50,validation_data=(X_test,y_test))

"""Here we see that the ANN predicted 18.6% of people to survived the incident."""

y_pred_ann = model.predict(test)

survived = 0
not_survived = 0
for i in range (0,418):
  if (y_pred_ann[i] >= 0.5) :
    survived = survived + 1
  else :
    not_survived = not_survived +1

total=418
p_survived = survived/total * 100
p_not_survived = not_survived/total * 100

print("Percentage of people predicted to survive",p_survived)
print("Percentage of people predicted to not survive",p_not_survived)

"""Below are a few plots to understand how the ann trained per epoch"""

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

"""### GridCV and Logistic Regression"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
parameters = [{'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['liblinear']},
              {'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['saga']},
              {'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['lbfgs']}]
grid_search=GridSearchCV(estimator=lr,
                         param_grid=parameters,
                         scoring='accuracy',
                         cv=10,
                         n_jobs=-1)

grid_search = grid_search.fit(X,y)

y_pred_lr = grid_search.predict(test)

"""The Logistic Regression predictes that only 38.2% of people to survived the incident."""

percentage(y_pred_lr)

"""### SVM"""

from sklearn.svm import SVC
svc = SVC()
svc.fit(X,y)

y_pred_svc=svc.predict(test)

"""The SVC predictes that only 3.8% of people survived the incident."""

percentage(y_pred_svc)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X,y)

y_pred_rf=rf.predict(test)

"""The Random Forest model predictes that 30.1% of people survived the incident."""

percentage(y_pred_rf)

"""### Passive Agressive Classifier"""

from sklearn.linear_model import PassiveAggressiveClassifier
PAC = PassiveAggressiveClassifier()
PAC.fit(X,y)

y_pred_pa=PAC.predict(test)

"""The Passive Agressive model predictes that 31.8% of people survived the incident."""

percentage(y_pred_pa)