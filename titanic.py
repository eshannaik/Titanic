# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13qbQbRhObXkso201Oi4BmtU26a-DcrLC

Titanic Survival Prediction

Importing and understanding the data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import keras
from keras.models import Model,Sequential
from keras.layers import Dense,Dropout

d = pd.read_csv("train_and_test2.csv")

d.head()

d.isnull().sum()

d.dropna(inplace=True)

d.isnull().sum()

d.info()

d.describe()

d.shape

d.nunique()

df = d[[col for col in d if d[col].nunique() > 1]]

o = d['2urvived'].isin([1]).sum()
z = d['2urvived'].isin([0]).sum()

"""### Visualization"""

l=["Did Not Survive","Survived"]
a=[z,o]
plt.pie(x=a,labels=l,autopct="%1.1f%%")
plt.show()

l1=["Female","Male"]
fig = plt.figure(figsize=(6,4))
ax = sns.countplot(df["Sex"], palette="Set3")

for p,label in zip(ax.patches,l1):
  ax.annotate(label,(p.get_x()+0.300,p.get_height()+0.50))

def dis(df):
  sns.displot(df)

dis(df['Age'])

dis(df['Pclass'])

corr = df.corr()
plt.figure(figsize=(10,5))
corMat = plt.matshow(corr,fignum=1)
plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.columns)), corr.columns)
plt.gca().xaxis.tick_bottom()
plt.colorbar(corMat)
plt.title(f'Correlation Matrix for Titanic dataset is', fontsize=15)
plt.show()

df.groupby('2urvived').hist(figsize=(18, 18))

"""### Models"""

X = df.iloc[:,0:7]
y =df.iloc[:,8]

X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.3)

model = Sequential()

model.add(Dense(units = 32, activation='relu', kernel_initializer = 'he_uniform', input_dim = 7))
model.add(Dense(units = 16, activation='relu', kernel_initializer = 'he_uniform'))
model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = 'glorot_uniform'))

model.compile(optimizer="adam",loss='binary_crossentropy',metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, batch_size = 128, epochs = 50,validation_data=(X_test,y_test))

model.evaluate(X_test,y_test)

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

"""###SVM"""

from sklearn.svm import SVC
svc = SVC()
svc.fit(X_train,y_train)

y_pred=svc.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm,annot=True,fmt='.2f')

from sklearn.metrics import accuracy_score
ac_svc = accuracy_score(y_test,y_pred)
ac_svc

"""###GridCV and Logistic Regression"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
parameters = [{'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['liblinear']},
              {'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['saga']},
              {'penalty' :['l1','l2','elasticnet'],'C': [1, 10, 100, 1000], 'solver': ['lbfgs']}]
grid_search=GridSearchCV(estimator=lr,
                         param_grid=parameters,
                         scoring='accuracy',
                         cv=10,
                         n_jobs=-1)

grid_search = grid_search.fit(X_train,y_train)

best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)

"""###Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,np.ravel(y_train))
y_pred=rf.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
cm_rf = confusion_matrix(y_test,y_pred)
sns.heatmap(cm_rf,annot=True,fmt='.2f')

from sklearn.metrics import accuracy_score
ac_rf = accuracy_score(y_test,y_pred)
ac_rf